{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15980fe6-5536-4f73-a264-a31c50204f64",
   "metadata": {},
   "source": [
    "# **L1 and L2 Norms**\n",
    "\n",
    "L1 and L2 norms are ways to measure the size or magnitude of a vector. They are commonly used in **machine learning, optimization, and regularization techniques** like Lasso and Ridge regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. L1 Norm (Manhattan Distance or Taxicab Norm)**\n",
    "The **L1 norm** of a vector is the **sum of the absolute values** of its components.  \n",
    "\n",
    "### **Formula**  \n",
    "For a vector $ x = (x_1, x_2, ..., x_n) $, the L1 norm is:  \n",
    "\n",
    "$$\n",
    "\\|x\\|_1 = |x_1| + |x_2| + \\dots + |x_n|\n",
    "$$\n",
    "\n",
    "### **Example**  \n",
    "If $ x = (3, -4) $, then:  \n",
    "\n",
    "$$\n",
    "\\|x\\|_1 = |3| + |-4| = 3 + 4 = 7\n",
    "$$\n",
    "\n",
    "### **Properties**  \n",
    "âœ… Encourages sparsity (used in Lasso regression).  \n",
    "âœ… Less sensitive to outliers than L2 norm.  \n",
    "âœ… Used in feature selection (removes less important coefficients).  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. L2 Norm (Euclidean Distance)**\n",
    "The **L2 norm** of a vector is the **square root of the sum of squares** of its components.  \n",
    "\n",
    "### **Formula**  \n",
    "For a vector $ x = (x_1, x_2, ..., x_n) $, the L2 norm is:  \n",
    "\n",
    "$$\n",
    "\\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\dots + x_n^2}\n",
    "$$\n",
    "\n",
    "### **Example**  \n",
    "If $ x = (3, -4) $, then:  \n",
    "\n",
    "$$\n",
    "\\|x\\|_2 = \\sqrt{3^2 + (-4)^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5\n",
    "$$\n",
    "\n",
    "### **Properties**  \n",
    "âœ… Used in Ridge regression to prevent large coefficients.  \n",
    "âœ… Penalizes large values more than L1 norm.  \n",
    "âœ… Sensitive to outliers.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Comparison**\n",
    "\n",
    "| Feature | L1 Norm (Manhattan) | L2 Norm (Euclidean) |\n",
    "|---------|------------------|----------------|\n",
    "| Formula | $ |x_1| + |x_2| + ... + |x_n| $ | $ \\sqrt{x_1^2 + x_2^2 + ... + x_n^2} $ |\n",
    "| Effect | Encourages sparsity | Shrinks all coefficients |\n",
    "| Use Case | Lasso regression, feature selection | Ridge regression, stability |\n",
    "| Sensitivity | Less sensitive to outliers | More sensitive to outliers |\n",
    "\n",
    "---\n",
    "\n",
    "## **Why is RMSE More Sensitive to Outliers Than MAE?**  \n",
    "\n",
    "Root Mean Squared Error (**RMSE**) and Mean Absolute Error (**MAE**) are both commonly used metrics for evaluating the performance of regression models, but **RMSE is more sensitive to outliers**. Hereâ€™s why:\n",
    "\n",
    "### **1. RMSE vs. MAE Formulas**  \n",
    "\n",
    "- **MAE (Mean Absolute Error):**  \n",
    "  $$\n",
    "  MAE = \\frac{1}{n} \\sum_{i=1}^{n} | y_i - \\hat{y}_i |\n",
    "  $$\n",
    "  - **Takes absolute errors** and averages them.  \n",
    "  - **Weights all errors equally** (linear impact).  \n",
    "\n",
    "- **RMSE (Root Mean Squared Error):**  \n",
    "  $$\n",
    "  RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "  $$\n",
    "  - **Squares the errors before averaging** and then takes the square root.  \n",
    "  - **Larger errors have a disproportionately higher impact** due to squaring.  \n",
    "\n",
    "### **2. The Effect of Outliers**  \n",
    "ðŸ‘‰ **Squaring in RMSE amplifies large errors.**  \n",
    "\n",
    "Letâ€™s say we have two prediction errors: **2 and 10**.  \n",
    "- **MAE Calculation:**  \n",
    "  $$\n",
    "  MAE = \\frac{|2| + |10|}{2} = \\frac{12}{2} = 6\n",
    "  $$\n",
    "- **RMSE Calculation:**  \n",
    "  $$\n",
    "  RMSE = \\sqrt{\\frac{(2^2) + (10^2)}{2}} = \\sqrt{\\frac{4 + 100}{2}} = \\sqrt{52} \\approx 7.21\n",
    "  $$\n",
    "\n",
    "### **Key Observation**  \n",
    "- A large error (10) increases **MAE by a small amount**.  \n",
    "- But it **drastically increases RMSE** due to squaring.  \n",
    "\n",
    "Thus, **RMSE penalises large errors more than MAE**, making it more sensitive to outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. When to Use MAE vs. RMSE**\n",
    "\n",
    "| Metric | Sensitivity | Use Case |\n",
    "|--------|------------|----------|\n",
    "| **MAE** | Less sensitive to outliers | When you want a simple, interpretable error measure (e.g., median-like behaviour) |\n",
    "| **RMSE** | More sensitive to outliers | When large errors should be penalised more (e.g., in applications like financial forecasting) |\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Code to Compare RMSE and MAE**\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Actual vs Predicted values (with an outlier)\n",
    "y_true = np.array([3, 5, 7, 9, 100])  # 100 is an outlier\n",
    "y_pred = np.array([2, 5, 6, 8, 10])\n",
    "\n",
    "# Compute MAE and RMSE\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "```\n",
    "\n",
    "### **Expected Output**\n",
    "```\n",
    "MAE: 17.20\n",
    "RMSE: 40.62\n",
    "```\n",
    "\n",
    "### **Key Observations**\n",
    "- **MAE** remains relatively low because it treats all errors **equally**.  \n",
    "- **RMSE** is much **higher** because it **squares** the error of the outlier (**100 vs. 10**) before averaging.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33826eb5-000c-453e-a599-10ac568cf34e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf169fa-150b-4e4f-b53b-b0dfff14e59b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fab84ba3-19b1-4a2e-aeef-63180f3edbb7",
   "metadata": {},
   "source": [
    "## Machine Learning Project Checklist\n",
    "\n",
    "1. Frame the problem and look at the big picture\n",
    "2. Get the data\n",
    "3. Explore the data to gain insights\n",
    "4. Prepare the data to better expose the underlying data patterns to machine learning algorithms.\n",
    "5. Explore many diffferent models and shortlist the best ones.\n",
    "6. Fine-tune your models and combine them into great solution.\n",
    "7. Present your solution\n",
    "8. Launch, monitor, and maintain your system.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66349222-bafa-4508-aa63-80760cd03adb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
