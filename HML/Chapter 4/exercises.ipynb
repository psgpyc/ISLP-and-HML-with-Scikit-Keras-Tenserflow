{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "730d6761-ca1f-4682-9210-fe2833bbd8eb",
   "metadata": {},
   "source": [
    "**1. Which linear regression training algoriothm you can use if you have a training set with millions of features?**  \n",
    "\n",
    "   We can use any of the linear regression training algorithm that uses gradient descent to find the optimal solution.  \n",
    "   - Batch Gradient Descent  \n",
    "   - Stocastic Gradient Descent  \n",
    "   - Mini Batch Gradient Descent\n",
    "\n",
    "   Closedform algorithms do not fare well when the feature set is larger. \n",
    "     \n",
    "**2. Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?**  \n",
    "    \n",
    "   All of the GD linear regression algorithm suffer from training set having diferent scales. We can use normalisation techniques to fit them in same scale.    \n",
    "  \n",
    "**3. Can gradient descent get stuck in a local minima when training a logistic regression model?**   \n",
    "\n",
    "   No. The cost function for logistic regression is a convex function. Therefore, there is only one global minima.  \n",
    "\n",
    "**4. Do all gradient descent algotihms lead to the same model, provided you let them run long enough?**   \n",
    "\n",
    "   No, stocastic and mini-batch gradient descent algorithms do not reach the optimal solutions but come really close to it.  We can make stocastic and mini-batch gradient descent to converge with proper learning rate decay.  \n",
    "\n",
    "\n",
    "**5. Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice that the validation error consitently goes up, what is likely going on? How can you fix it?**    \n",
    "\n",
    "   THis is a classic example of model overfitting the data. We can use regularisation or early stopping.  \n",
    "\n",
    "**6. Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up?**    \n",
    "\n",
    "   Yes.  If the training MSE keeps going down but Validation Error starts to go up then thats the sign of overfitting and we can stop.\n",
    "\n",
    "**7. Which gradient descent algorithm (among those we discussed) will reach the optimal solution fastest? Which will actually converge?  How can you make the others converge as well?**    \n",
    "\n",
    "   The stocastic gradeint descent will reach optimal solution faster(when used with decay). Batch gradient descent will actually converse but is slower. Mini batch offers the best of both solutions. We can make others converge aswell by using complete data points.  \n",
    "\n",
    "\n",
    "**8. Suppose you are using polynomial regression. You plot the learning curve and you notice there is a large gap between the training error and the validation error. What is happening? What are the three ways to solve this?**    \n",
    "    The model is overfitting the data. The three ways to solve this is:  \n",
    "  \n",
    "   - We can use simpler model  \n",
    "   - We can increase the size of training set  \n",
    "   - Regularisation  \n",
    "\n",
    "**9. Suppose you are using the ridge regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularisation hyperparameter or reduce it?**    \n",
    "\n",
    "    The model suffers from high bias. We should reduce the hypterparameter $\\alpha$  \n",
    "\n",
    "**10. Why would you want to use:**  \n",
    "\n",
    "    a. Ridge regression indead of plain linear regression (i.e. without any regularisation): To reduce overfitting of the data. I.e. it forces the models to keep weights to the lowest.    \n",
    "\n",
    "    b. Lasso instead of ridge regression: Lasso is used when we have a lot of unwanted features and want to reduce their effect on the model. Lasso regression reduces the parameters of less useful features to zero thus reducing their effect on the model.  \n",
    "\n",
    "    c. Elastic net instead of lasso regression: When no. of features is greater than no. of training examples or several features are strongly correlated.    \n",
    "\n",
    "**11. Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you impelement two logistic regression classifiers or one softmax regression classifier?**\n",
    "\n",
    "    I would implement two logistic regression classifier.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55c3ec6-4783-42fa-9d64-1cc02c37dca3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
