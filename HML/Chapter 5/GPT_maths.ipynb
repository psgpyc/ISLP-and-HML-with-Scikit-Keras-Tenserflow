{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c40486-525e-40f0-9e50-e80b690da4b9",
   "metadata": {},
   "source": [
    "# Mathematics Behind Hard Margin SVM\n",
    "\n",
    "This document explains the mathematical formulation of the **hard margin Support Vector Machine (SVM)**, which is used when the training data are linearly separable.\n",
    "\n",
    "## 1. The Goal: Maximizing the Margin\n",
    "\n",
    "Given a training set of $n$ data points:\n",
    "$$\n",
    "\\{(\\mathbf{x}_i, y_i)\\}_{i=1}^n, \\quad \\text{with } \\mathbf{x}_i \\in \\mathbb{R}^d \\text{ and } y_i \\in \\{-1, +1\\},\n",
    "$$\n",
    "we want to find a hyperplane that separates the two classes. This hyperplane is given by:\n",
    "$$\n",
    "\\mathbf{w}^\\top \\mathbf{x} + b = 0.\n",
    "$$\n",
    "\n",
    "### The Margin\n",
    "\n",
    "The **margin** is the distance between the hyperplane and the nearest data points from either class. The distance of a point $ \\mathbf{x} $ from the hyperplane is:\n",
    "$$\n",
    "\\text{Distance} = \\frac{|\\mathbf{w}^\\top \\mathbf{x} + b|}{\\|\\mathbf{w}\\|}.\n",
    "$$\n",
    "\n",
    "For the closest point, the margin $M$ is:\n",
    "$$\n",
    "M = \\min_i \\frac{|\\mathbf{w}^\\top \\mathbf{x}_i + b|}{\\|\\mathbf{w}\\|}.\n",
    "$$\n",
    "\n",
    "Since the scale of $\\mathbf{w}$ and $b$ is arbitrary, we can rescale them such that for the support vectors (the points closest to the hyperplane):\n",
    "$$\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) = 1.\n",
    "$$\n",
    "\n",
    "With this normalization, the margin becomes:\n",
    "$$\n",
    "M = \\frac{1}{\\|\\mathbf{w}\\|}.\n",
    "$$\n",
    "\n",
    "Thus, **maximizing the margin** is equivalent to **minimizing** $\\|\\mathbf{w}\\|$ (or, for convenience, $\\frac{1}{2}\\|\\mathbf{w}\\|^2$).\n",
    "\n",
    "## 2. Formulating the Optimization Problem\n",
    "\n",
    "The optimization problem for the hard margin SVM is formulated as follows:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\min_{\\mathbf{w},\\, b} \\quad & \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\\\\n",
    "\\text{subject to} \\quad & y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1,\\quad \\text{for } i = 1,\\dots,n.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- **Objective Function:** $\\frac{1}{2}\\|\\mathbf{w}\\|^2$ â€” minimizing this is equivalent to maximizing the margin.\n",
    "- **Constraints:** Ensure that all points are correctly classified with a margin of at least 1.\n",
    "\n",
    "## 3. Setting Up the Lagrangian\n",
    "\n",
    "To solve the constrained optimization problem, we introduce Lagrange multipliers $\\alpha_i \\ge 0$ (one for each constraint) and form the Lagrangian:\n",
    "$$\n",
    "L(\\mathbf{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2}\\|\\mathbf{w}\\|^2 - \\sum_{i=1}^{n} \\alpha_i \\left[ y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) - 1 \\right].\n",
    "$$\n",
    "\n",
    "## 4. Deriving the Dual Problem\n",
    "\n",
    "### a. Stationarity Conditions\n",
    "\n",
    "To find the optimum, we set the derivatives of $L$ with respect to $\\mathbf{w}$ and $b$ to zero.\n",
    "\n",
    "1. **Derivative with respect to $\\mathbf{w}$:**\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i = \\mathbf{0},\n",
    "   $$\n",
    "   which gives:\n",
    "   $$\n",
    "   \\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i.\n",
    "   $$\n",
    "\n",
    "2. **Derivative with respect to $b$:**\n",
    "   $$\n",
    "   \\frac{\\partial L}{\\partial b} = -\\sum_{i=1}^{n} \\alpha_i y_i = 0,\n",
    "   $$\n",
    "   leading to:\n",
    "   $$\n",
    "   \\sum_{i=1}^{n} \\alpha_i y_i = 0.\n",
    "   $$\n",
    "\n",
    "### b. Forming the Dual Function\n",
    "\n",
    "Substitute $\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i y_i \\mathbf{x}_i$ back into the Lagrangian. After simplifying, the dual problem becomes:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_{\\boldsymbol{\\alpha}} \\quad & \\sum_{i=1}^{n} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{n}\\sum_{j=1}^{n} \\alpha_i \\alpha_j y_i y_j\\, \\mathbf{x}_i^\\top \\mathbf{x}_j \\\\\n",
    "\\text{subject to} \\quad & \\sum_{i=1}^{n} \\alpha_i y_i = 0, \\\\\n",
    "& \\alpha_i \\ge 0,\\quad i = 1,\\dots,n.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is a convex quadratic programming problem in terms of $\\boldsymbol{\\alpha}$.\n",
    "\n",
    "### c. Recovering $\\mathbf{w}$ and $b$\n",
    "\n",
    "Once the optimal $\\alpha_i^\\ast$ are determined, the weight vector $\\mathbf{w}$ is computed as:\n",
    "$$\n",
    "\\mathbf{w} = \\sum_{i=1}^{n} \\alpha_i^\\ast y_i \\mathbf{x}_i.\n",
    "$$\n",
    "\n",
    "The bias $b$ can be determined using any support vector (i.e., a point for which $\\alpha_i^\\ast > 0$) by solving:\n",
    "$$\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) = 1.\n",
    "$$\n",
    "\n",
    "## 5. Karush-Kuhn-Tucker (KKT) Conditions\n",
    "\n",
    "For optimality, the following KKT conditions must hold:\n",
    "\n",
    "1. **Primal Feasibility:**\n",
    "   $$\n",
    "   y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) - 1 \\ge 0 \\quad \\text{for all } i.\n",
    "   $$\n",
    "\n",
    "2. **Dual Feasibility:**\n",
    "   $$\n",
    "   \\alpha_i \\ge 0 \\quad \\text{for all } i.\n",
    "   $$\n",
    "\n",
    "3. **Complementary Slackness:**\n",
    "   $$\n",
    "   \\alpha_i \\left( y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) - 1 \\right) = 0 \\quad \\text{for all } i.\n",
    "   $$\n",
    "   This means that if $\\alpha_i > 0$ (i.e., the point is a support vector), then the corresponding constraint is exactly satisfied.\n",
    "\n",
    "4. **Stationarity:**\n",
    "   As shown by setting the gradients of the Lagrangian with respect to $\\mathbf{w}$ and $b$ to zero.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Objective:** Maximize the margin (equivalently, minimize $\\frac{1}{2}\\|\\mathbf{w}\\|^2$).\n",
    "- **Constraints:** Ensure every data point satisfies:\n",
    "$$\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b) \\ge 1.\n",
    "$$\n",
    "- **Lagrangian and Dual Formulation:** Use Lagrange multipliers $\\alpha_i \\ge 0$ to derive the dual problem.\n",
    "- **KKT Conditions:** Guarantee optimality and determine the support vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd5be2e-37f4-49f4-bee2-6af183cddf1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
