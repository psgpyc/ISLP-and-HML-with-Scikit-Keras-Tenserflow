{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cedefd98-fe64-4d3d-908b-07fe85f0f778",
   "metadata": {},
   "source": [
    "### Steps for traiming a NN\n",
    "\n",
    "**Step 1: Define a Model**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=25, activation=\"Sigmoid\"),\n",
    "    Dense(units=15, activation=\"Sigmoid\"),\n",
    "    Dense(units=1, activation=\"Sigmoid\")\n",
    "])\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "**Step 2: Choose a Loss/Cos Function**\n",
    "```python\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "model.compile(loss=BinaryCrossentropy)\n",
    "\n",
    "```\n",
    "**Step 3: Fit the Model**\n",
    "```python\n",
    "\n",
    "model.fit(X,y, epochs=100)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a02330-e93b-42e4-b776-7b139ed0d464",
   "metadata": {},
   "source": [
    "### What the code is actually doing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aed8ba-869e-4a0b-9fc2-6410003598bc",
   "metadata": {},
   "source": [
    "#### Let us first recall what actually happend with a logistic regression algorithm. \n",
    "\n",
    "1. We had our linear function `z` and the sigmoid function `f_x`: **Forward Pass**   \n",
    "   `z = np.dot(W, X) + b`  \n",
    "    `f_x = 1/(1+np.exp(-z))`\n",
    "\n",
    "   ***This step corresponds to the Step 1 of the above example where we used TF***\n",
    "3. We define our loss function: $L(f_{\\vec w, b}{(\\vec x), y})$\n",
    "   \n",
    "   `loss = -y * np.log(f_x)- (1-y) * np.log(1-f_x)`\n",
    "   \n",
    "   ***This step corresponds to the Step 2 of the above example where we used TF***\n",
    "\n",
    "5. We then have our cost function: $J(\\vec w, b) = \\frac{1}{m} \\sum_{i=1}^{m} L(f_{\\vec w,b}{(\\vec x^{(i)}, y^{(i)})})$\n",
    "\n",
    "   `J_WB = np.mean(loss-y)`\n",
    "\n",
    "   ***Having specified the Loss function, the TF automatically figures out the cost function***\n",
    "\n",
    "\n",
    "7. We then used gradient descent to minimse the cost function and learn the value of $\\vec w$ and $b$.  \n",
    "   `w = w- alpha * dj_dw`  \n",
    "   `b = b- alpha ** dj_db`\n",
    "\n",
    "   ***This step corresponds to the Step 3 of the above example where we used TF. The `model.fit()` will do the back propagation***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab44c9f-8b15-4ab9-a016-85924dc82dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4b3212-f888-4024-8fad-52282db94df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
