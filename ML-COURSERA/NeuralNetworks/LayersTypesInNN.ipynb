{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5679bf66-02d2-4980-b8c9-30dfb65aa474",
   "metadata": {},
   "source": [
    "# Types of Layers in Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Input Layer**\n",
    "### **Purpose**:\n",
    "   - Acts as the entry point for the raw input data into the network. It doesn’t perform any computation but forwards the data to the subsequent layers.\n",
    "\n",
    "### **Structure**:\n",
    "   - The number of neurons (nodes) corresponds to the number of features in the input data.\n",
    "   - For example:\n",
    "     - In image data, if the input is a 28×28 grayscale image, the input layer will have $28 \\times 28 = 784$ neurons.\n",
    "     - For tabular data, if there are 10 features, the input layer will have 10 neurons.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Dense (Fully Connected) Layer**\n",
    "### **Purpose**:\n",
    "   - Connects every neuron in one layer to every neuron in the next layer.\n",
    "   - Learns weights for every connection and applies biases and activation functions.\n",
    "\n",
    "### **Mechanics**:\n",
    "   - The output of a neuron is calculated as:\n",
    "     $y = \\sigma\\left(\\sum_{i=1}^n w_i x_i + b\\right)$\n",
    "     where:\n",
    "     - $w_i$: Weight for the $i$-th input.\n",
    "     - $x_i$: Value of the $i$-th input.\n",
    "     - $b$: Bias.\n",
    "     - $\\sigma$: Activation function.\n",
    "\n",
    "### **Applications**:\n",
    "   - Used in feedforward neural networks for tasks like classification and regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Activation Layer**\n",
    "### **Purpose**:\n",
    "   - Introduces non-linearity into the network, enabling it to learn complex patterns and relationships in the data.\n",
    "\n",
    "### **Common Activation Functions**:\n",
    "1. **ReLU (Rectified Linear Unit)**:\n",
    "   - Formula: $f(x) = \\max(0, x)$.\n",
    "   - Benefits:\n",
    "     - Avoids vanishing gradient problems.\n",
    "     - Computationally efficient.\n",
    "   - Drawback:\n",
    "     - Can cause dead neurons if weights update to negative values.\n",
    "\n",
    "2. **Sigmoid**:\n",
    "   - Formula: $f(x) = \\frac{1}{1 + e^{-x}}$.\n",
    "   - Benefits:\n",
    "     - Squashes output to the range (0, 1), suitable for probabilities.\n",
    "   - Drawback:\n",
    "     - Prone to vanishing gradient issues for large positive/negative inputs.\n",
    "\n",
    "3. **Tanh (Hyperbolic Tangent)**:\n",
    "   - Formula: $f(x) = \\tanh(x)$.\n",
    "   - Benefits:\n",
    "     - Outputs in range (-1, 1), making it zero-centered.\n",
    "   - Drawback:\n",
    "     - Similar vanishing gradient issues as sigmoid.\n",
    "\n",
    "4. **Softmax**:\n",
    "   - Formula: $f(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$.\n",
    "   - Benefits:\n",
    "     - Converts logits into probabilities.\n",
    "     - Commonly used in multi-class classification.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Convolutional Layer**\n",
    "### **Purpose**:\n",
    "   - Extracts spatial and hierarchical features from data, particularly effective for image and video processing.\n",
    "\n",
    "### **Mechanics**:\n",
    "   - Applies convolution operations using filters (kernels).\n",
    "   - A filter slides over the input data, performing element-wise multiplication and summation:\n",
    "     $(I * K)[i, j] = \\sum_{m=0}^{M} \\sum_{n=0}^{N} I[i+m, j+n] \\cdot K[m, n]$\n",
    "     where:\n",
    "     - $I$: Input matrix.\n",
    "     - $K$: Kernel matrix.\n",
    "     - $M, N$: Dimensions of the kernel.\n",
    "\n",
    "### **Parameters**:\n",
    "   - **Kernel Size**: Dimensions of the filter (e.g., 3×3, 5×5).\n",
    "   - **Stride**: Number of steps the filter moves at each step.\n",
    "   - **Padding**:\n",
    "     - **Valid**: No padding; output shrinks.\n",
    "     - **Same**: Padding added to maintain dimensions.\n",
    "\n",
    "### **Applications**:\n",
    "   - Image classification, object detection, and style transfer.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Pooling Layer**\n",
    "### **Purpose**:\n",
    "   - Reduces spatial dimensions of feature maps, lowering computational cost and mitigating overfitting.\n",
    "\n",
    "### **Types**:\n",
    "1. **Max Pooling**:\n",
    "   - Extracts the maximum value in a pooling window.\n",
    "2. **Average Pooling**:\n",
    "   - Computes the average value in a pooling window.\n",
    "\n",
    "### **Parameters**:\n",
    "   - Pooling size (e.g., 2×2, 3×3).\n",
    "   - Stride.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Recurrent Layer**\n",
    "### **Purpose**:\n",
    "   - Processes sequential data by retaining information about previous inputs through hidden states.\n",
    "\n",
    "### **Types**:\n",
    "1. **Simple RNN**:\n",
    "   - Computes the hidden state at time $t$ as:\n",
    "     $h_t = \\sigma(W_h h_{t-1} + W_x x_t + b)$\n",
    "   - Struggles with long-term dependencies.\n",
    "\n",
    "2. **LSTM (Long Short-Term Memory)**:\n",
    "   - Handles long-term dependencies using gates:\n",
    "     - Forget gate: Decides which information to discard.\n",
    "     - Input gate: Decides which information to store.\n",
    "     - Output gate: Decides what to output.\n",
    "\n",
    "3. **GRU (Gated Recurrent Unit)**:\n",
    "   - Similar to LSTM but with fewer gates.\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Dropout Layer**\n",
    "### **Purpose**:\n",
    "   - Reduces overfitting by randomly deactivating a fraction of neurons during training.\n",
    "\n",
    "### **Mechanics**:\n",
    "   - During training:\n",
    "     - A neuron is kept active with a probability $p$.\n",
    "     - Otherwise, its output is set to 0.\n",
    "   - At inference:\n",
    "     - Outputs are scaled by $p$ to maintain consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Batch Normalisation Layer**\n",
    "### **Purpose**:\n",
    "   - Normalises the inputs of each layer to stabilise training and improve convergence speed.\n",
    "\n",
    "### **Mechanics**:\n",
    "   - Normalises inputs to have zero mean and unit variance:\n",
    "     $\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}$\n",
    "   - Then scales and shifts using learnable parameters $\\gamma$ and $\\beta$:\n",
    "     $y = \\gamma \\hat{x} + \\beta$\n",
    "\n",
    "---\n",
    "\n",
    "## **9. Attention Layer**\n",
    "### **Purpose**:\n",
    "   - Focuses on specific parts of the input data, dynamically assigning importance to different features.\n",
    "\n",
    "### **Applications**:\n",
    "   - Transformer models for NLP and computer vision.\n",
    "\n",
    "---\n",
    "\n",
    "## **10. Embedding Layer**\n",
    "### **Purpose**:\n",
    "   - Converts discrete categorical variables into dense continuous representations.\n",
    "\n",
    "### **Mechanics**:\n",
    "   - Represents words or categories in a lower-dimensional vector space.\n",
    "   - Example:\n",
    "     - Word embedding: \"king\" → [0.12, 0.89, 0.33].\n",
    "\n",
    "---\n",
    "\n",
    "## **11. Residual Layer**\n",
    "### **Purpose**:\n",
    "   - Introduces skip connections in deep networks to alleviate vanishing gradient problems.\n",
    "   - Output of a residual block:\n",
    "     $y = F(x) + x$\n",
    "\n",
    "---\n",
    "\n",
    "## **12. Output Layer**\n",
    "### **Purpose**:\n",
    "   - Produces the final output of the model.\n",
    "   - For regression: Single neuron (e.g., predicting continuous values).\n",
    "   - For classification: One neuron per class with a Softmax activation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Custom Layers**\n",
    "- Created to handle specific tasks using frameworks like TensorFlow or PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "These layers form the foundation for designing sophisticated neural network architectures tailored for diverse applications like image recognition, natural language processing, and time series prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d3596-c0ed-44f9-aa70-85fe6bbeea4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
