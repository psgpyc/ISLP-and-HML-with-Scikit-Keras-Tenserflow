{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b001fa-e305-4cfb-a3b3-c05e8e4e165f",
   "metadata": {},
   "source": [
    "## Reinforcement Learning\n",
    "\n",
    "We tell the algorithm what to do rather than how to do it. And build a reward system to either suppot the action(Positive Reward) or penalise it(Negative Reward)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d50993-225e-44a8-aa0b-58d062ef8fe0",
   "metadata": {},
   "source": [
    "<img src=\"./images/mars-rover-example.png\" width=\"500\">\n",
    "\n",
    "The Mars rover operates in six states (positions) labeled 1 to 6. It starts in state 4 and can move left or right. State 1 is the most scientifically valuable with a reward of 100, state 6 has moderate value with a reward of 40, and states 2, 3, 4, and 5 offer no rewards. The rover’s goal is to maximize rewards by determining the best sequence of moves.\n",
    "\n",
    "\n",
    "If we want the robot to do something that is important, we can do show by associating it with high reward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9d643-b889-45e6-93f8-a18b79ec3b52",
   "metadata": {},
   "source": [
    "Each state has a reward: state 1 offers 100, state 6 offers 40, and states 2–5 offer 0. At each step, the rover chooses an action (left or right) to move to a new state. The key elements of reinforcement learning are the current state (S), action taken, reward (R(S)) from the current state, and the next state (S’). The goal is to maximise cumulative rewards through optimal actions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930b8d1f-74a7-488d-8818-66b30368d513",
   "metadata": {},
   "source": [
    "## Return in Reinforcement Learning\n",
    "\n",
    "How do to determine which set of reward is better?\n",
    "\n",
    "As we go through this, one analogy that you might find helpful is if you imagine you have a five-dollar bill at your feet, you can reach down and pick up, or half an hour across town, you can walk half an hour and pick up a 10-dollar bill. Which one would you rather go after? Ten dollars is much better than five dollars, but if you need to walk for half an hour to go and get that 10-dollar bill, then maybe it'd be more convenient to just pick up the five-dollar bill instead. The concept of a return captures that rewards you can get quicker are maybe more attractive than rewards that take you a long time to get to. Let's take a look at exactly how that works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a64753f-e361-4c5e-b68d-0549be04d397",
   "metadata": {},
   "source": [
    "Let us conider,\n",
    "\n",
    "discount factor $(\\gamma)$ = $0.9$  \n",
    "state = $i , \\text{where} \\ i = 0,1,2,3,4.....$  \n",
    "Return at at state = $R_i$  \n",
    "Therefore the return is calulcated as,   \n",
    "\n",
    "Return = $R_1 + \\gamma R_2 + \\gamma^2 R_3 + \\gamma^3 R_4 + ....$\n",
    "\n",
    "The discont factor makes the algorithm a little bit impatience. It gives a full return if you achive reward in the first state. Further it goes, it reduces the return.\n",
    "\n",
    "\n",
    "#### Larger discount factor like 0.5\n",
    "\n",
    "I'm actually going to use a discount factor of 0.5. This very heavily down weights or very heavily we say discounts rewards in the future, because with every additional passing timestamp, you get only half as much credit as rewards that you would have gotten one step earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a0fd24-0223-42b5-897f-449e13b9bdae",
   "metadata": {},
   "source": [
    "<img src=\"./images/discountfactor.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14febfc4-4888-4414-815c-db64e54da4af",
   "metadata": {},
   "source": [
    "In the first example, we are only moving to the left, So if you are in step 4,   \n",
    "\n",
    "the reward you will receive is:\n",
    "\n",
    "Return_from_state_4 = $ 0.5 \\times 0 + 0.25 \\times 0 + 0.125 \\times 100 = 12.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8210b9b0-7e38-43ad-92b0-53ef7ff4494f",
   "metadata": {},
   "source": [
    "For systems with negative rewards, it causes the algorithm to try to push out the make the rewards as far into the future as possible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07163f51-99ef-4192-b74b-4d242c6bbd4b",
   "metadata": {},
   "source": [
    "## Policies in Reinforcement Learning\n",
    "\n",
    "How to choose the actions?\n",
    "\n",
    "1. Always for nearer reward\n",
    "2. Always go for rewards with larger return\n",
    "\n",
    "our goal is to come up with a function which is called a policy $\\pi$, whose job it is to take as input any state s and map it to some action a that it wants us to take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd32e7b5-7bc0-4379-a19c-790ad8839067",
   "metadata": {},
   "source": [
    "**The goal of reinforcement learning algorith is to find a policy $\\pi$ that tells you what action $a = \\pi(s)$ to take in every state (s) so as to maximize the return.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69336093-cade-4832-8360-69a321ffb3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
