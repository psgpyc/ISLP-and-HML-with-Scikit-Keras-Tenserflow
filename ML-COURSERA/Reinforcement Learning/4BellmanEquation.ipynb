{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8f4ae75-879b-44bf-94cb-83ebccb6d895",
   "metadata": {},
   "source": [
    "$Q(s,a)$ = Return if you  \n",
    "             - start in state $s$  \n",
    "                - take action $a$ (once)  \n",
    "                - then behave optimally after that  \n",
    "\n",
    "How do we calculate $Q(s,a)$\n",
    "\n",
    "Let,  \n",
    "$s$ = Current State  \n",
    "$R(s)$ = reward of current state  \n",
    "$a$ = current action  \n",
    "$s'$ = state you get to after taking a action $a$  \n",
    "$a'$ = action that you take in state $s'$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67387cc9-0a15-491e-ab7b-c2fdf6c51df5",
   "metadata": {},
   "source": [
    "## Bellman Equation\n",
    "\n",
    "$Q(s,a) = R(s) + \\gamma  \\times max_{a'} Q(s',a')$\n",
    "\n",
    "Consider the below, \n",
    "\n",
    "\n",
    "<img src=\"./images/bellmen.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057202d4-1343-4499-83fd-5c57c0976a5d",
   "metadata": {},
   "source": [
    "$Q(2, right) = R(2) + 0.5 \\times Q(3, a') = 0 + 0.5 \\times 25 = 12.5$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4417289-ca5c-47a3-89b3-8d0baca9b69e",
   "metadata": {},
   "source": [
    "## *further study on stochastic environment and reinforcement learning*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04bfcb5-170d-4963-81dd-cc37800308e0",
   "metadata": {},
   "source": [
    "Imagine you’re playing a game where you have to make choices, like moving around a board or picking up toys. Each time you make a choice, you get a reward, like points or coins. But here’s the catch: the more you play, the better your choices can be if you think about both the rewards you get right now and the rewards you might get later.\n",
    "\n",
    "The Bellman equation is like a rule that helps you figure out the best way to make these choices, so you get the most rewards over time.\n",
    "\n",
    "It says:  \n",
    "\t1.\t“If you’re in a particular place (or state), and you make a choice (or action), you get an immediate reward.”  \n",
    "\t2.\t“But don’t just think about this reward. Think about the rewards you can get later, if you keep making good choices.”  \n",
    "\n",
    "To do this, the Bellman equation helps you break everything down:  \n",
    "\t•\tYou first get the reward for the choice you made right now.  \n",
    "\t•\tThen, you think about where you could go next (the next place) and how much reward that place might give you if you keep playing well.  \n",
    "\n",
    "So, it’s like planning ahead and making sure you make the best choice now, knowing that it will lead to better choices and rewards in the future!   \n",
    "\n",
    "This rule helps you play better and smarter in the game, always choosing the best actions to get the most rewards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2642626-4524-4c84-ab5d-21e1a11022ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
