{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8126fd02-0ad0-4a36-9af8-2584f23498b0",
   "metadata": {},
   "source": [
    "# Summary of Key Concepts in Reinforcement Learning and Markov Decision Processes (MDP)\n",
    "\n",
    "## 1. Reinforcement Learning Framework\n",
    "- **Components**:\n",
    "  - **States**: Represent different situations or positions in a problem.\n",
    "  - **Actions**: Choices the agent can make to transition between states.\n",
    "  - **Rewards**: Feedback signal indicating the immediate benefit of an action.\n",
    "  - **Discount Factor (γ)**: A number (0 ≤ γ ≤ 1) determining how much future rewards are valued compared to immediate rewards.\n",
    "  - **Return**: The cumulative reward obtained over time, discounted by γ.\n",
    "  - **Policy (π)**: A strategy that maps states to actions to maximize the return.\n",
    "\n",
    "## 2. Mars Rover Example\n",
    "- **States**: Six states numbered 1 to 6.\n",
    "- **Actions**: \"Go left\" or \"go right\".\n",
    "- **Rewards**: \n",
    "  - 100 for the leftmost state.\n",
    "  - 40 for the rightmost state.\n",
    "  - 0 for intermediate states.\n",
    "- **Discount Factor (γ)**: 0.5.\n",
    "- **Return**: Calculated using the reward and discount factor.\n",
    "- **Policy (π)**: Determines actions based on the current state to maximize the return.\n",
    "\n",
    "## 3. Application of Reinforcement Learning to Other Domains\n",
    "\n",
    "### Autonomous Helicopter\n",
    "- **States**: Positions, orientations, speeds, etc., of the helicopter.\n",
    "- **Actions**: Possible ways to move the helicopter's control stick.\n",
    "- **Rewards**:\n",
    "  - +1 if the helicopter flies well.\n",
    "  - -1,000 if it crashes.\n",
    "- **Discount Factor (γ)**: Typically a high value like 0.99.\n",
    "- **Return**: Computed using rewards and the discount factor.\n",
    "- **Policy (π)**: Provides the optimal control stick movements for each state.\n",
    "\n",
    "### Chess\n",
    "- **States**: Positions of pieces on the board (simplified representation).\n",
    "- **Actions**: All possible legal moves in the game.\n",
    "- **Rewards**:\n",
    "  - +1 for a win.\n",
    "  - -1 for a loss.\n",
    "  - 0 for a tie.\n",
    "- **Discount Factor (γ)**: Very close to 1, such as 0.99, 0.995, or 0.999.\n",
    "- **Return**: Calculated similarly as in other applications.\n",
    "- **Policy (π)**: Selects the best move given a board position.\n",
    "\n",
    "## 4. Markov Decision Process (MDP)\n",
    "\n",
    "<img src=\"./images/markovdp.png\" width=\"500\">\n",
    "\n",
    "- **Definition**: A formalism for reinforcement learning problems with states, actions, rewards, and policies.\n",
    "- **Markov Property**: The future depends only on the current state and not on the sequence of states leading up to it.\n",
    "- **Key Components**:\n",
    "  - **Agent**: Chooses actions based on the policy (π).\n",
    "  - **Environment**: Changes state and provides rewards based on the agent's actions.\n",
    "  - **Cycle**:\n",
    "    1. Agent chooses an action.\n",
    "    2. Environment transitions to a new state and provides a reward.\n",
    "    3. Agent observes the new state and reward.\n",
    "\n",
    "## 5. Diagram Representation\n",
    "- Visual diagrams illustrate the flow of states, actions, rewards, and the interaction between the agent and the environment.\n",
    "\n",
    "## 6. Next Steps\n",
    "- **State-Action Value Function**: A key concept for developing algorithms to select optimal actions.\n",
    "- **Goal**: Learn how to define and compute the state-action value function, which is foundational for reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c6f0dc-f4c8-4f9c-ad5c-3d9dec9d9653",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
