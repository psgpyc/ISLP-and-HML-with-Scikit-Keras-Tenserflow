{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0134b10b-bd7d-44f8-a38f-b90ec5b4abc7",
   "metadata": {},
   "source": [
    "## Evaluate your model\n",
    "\n",
    "\n",
    "It is difficult to visualise the fit of the model when we have features greater than 2. \n",
    "\n",
    "Thererfore, we use train-test split to evaluate the performance of our model.\n",
    "\n",
    "1. Fit the model into train set.\n",
    "2. Use the model in test set and find the squared sum of error in the test set.\n",
    "\n",
    "We might see, $J_{train}(\\vec w, b)$ is less than $J_{test}(\\vec w, b)$. This is an example of overfitting where our algorithm failed to generalise.\n",
    "\n",
    "\n",
    "In **classification** setting, we evaluate by counting the number of misclassified prediction.\n",
    "\n",
    "$\\text{count}( \\hat y \\neq y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd02791-afe6-4dd9-99ac-79d26a3ab3ee",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We might choose one of these models:\n",
    "\n",
    "1. $f_{\\vec w, b} \\vec x = W_1x +b$\n",
    "2. $f_{\\vec w, b} \\vec x = W_1x + W_2x^2 +b$\n",
    "3. $f_{\\vec w, b} \\vec x = W_1x + W_2x^2 + w_3x^3+b$\n",
    "4. ........\n",
    "\n",
    "We can either choose a lienar model like in $(1)$ or quadratic like in $(2)$ or higher order polynomial like in $(3)$. We might keep implementing higher order polynomial.\n",
    "\n",
    "\n",
    "**One way of choosing a model might be** trying out each model (may be upto $10^{th}$ degree polynomial) and see which gives us low cost.\n",
    "\n",
    "The issue with this is similar to over-fitting in the train set. We might see that the cost is less than the train set. This can be an overly optimistic view. \n",
    "\n",
    "**How to decide which degree of polynomial to use?**\n",
    "\n",
    "> Instead of splitting the data into just *train and test set*, we will also use a **validation set**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae34a8-6369-44fa-9211-38208849f3d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d05d995-752c-4065-a854-c769b335969a",
   "metadata": {},
   "source": [
    "# Overfitting and Higher-Order Polynomials\n",
    "\n",
    "## **Model Complexity**\n",
    "\n",
    "- A **linear model** (e.g., $$f(x) = W_1x + b$$) is simple and captures linear relationships. It may not capture the complexity of a dataset with nonlinear patterns.\n",
    "- A **quadratic model** (e.g., $$f(x) = W_1x + W_2x^2 + b$$) or higher-order polynomials can capture more complex patterns.\n",
    "- As the degree of the polynomial increases, the model becomes more flexible and can fit the training data more closely.\n",
    "\n",
    "## **Overfitting**\n",
    "\n",
    "- When we fit a very high-order polynomial (e.g., $10^{th}$-degree), the model might capture not only the true underlying pattern in the data but also the noise.\n",
    "- This leads to **overfitting**, where the model performs exceptionally well on the training set but poorly on unseen data (test set). Overfitting gives a false sense of accuracy since the model essentially \"memorizes\" the training data.\n",
    "\n",
    "## **Symptoms of Overfitting**\n",
    "\n",
    "- The training error (or cost) is very low, but the test error is high.\n",
    "- The model might display erratic or extreme fluctuations, especially in regions of the input space where there is little or no data.\n",
    "\n",
    "## **Why Overfitting Happens in Higher-Order Polynomials**\n",
    "\n",
    "When you use higher-order polynomials:\n",
    "\n",
    "1. The model has more parameters ($W_1, W_2, \\dots, W_k$), which increases its capacity to fit the data.\n",
    "2. The polynomial terms (e.g., $x^3, x^4, \\dots, x^{10}$) create complex curves that can pass through almost all points in the training set, regardless of whether those points represent meaningful trends or random noise.\n",
    "\n",
    "While this reduces the cost on the training set, it does not generalize well to new data because the high-degree polynomial captures noise rather than the true signal.\n",
    "\n",
    "## **How to Choose a Model Without Overfitting**\n",
    "\n",
    "To mitigate overfitting and select the best model:\n",
    "\n",
    "### **1. Use a Validation Set**\n",
    "\n",
    "- Split the dataset into three parts: training, validation, and test sets.\n",
    "- Train the model on the training set and evaluate its performance on the validation set.\n",
    "- Choose the degree of the polynomial that minimizes the validation error, not just the training error.\n",
    "\n",
    "### **2. Regularization**\n",
    "\n",
    "- Apply techniques like $L_2$ regularization (Ridge Regression) or $L_1$ regularization (Lasso Regression) to penalize large coefficients of higher-order terms. This helps prevent overfitting by discouraging overly complex models.\n",
    "\n",
    "### **3. Cross-Validation**\n",
    "\n",
    "- Use k-fold cross-validation to get a more reliable estimate of model performance. This reduces the risk of choosing a model that happens to perform well on a specific validation split due to randomness.\n",
    "\n",
    "### **4. Bias-Variance Tradeoff**\n",
    "\n",
    "Understand the tradeoff:\n",
    "\n",
    "- **Low-degree polynomial**: High bias, low variance (underfitting).\n",
    "- **High-degree polynomial**: Low bias, high variance (overfitting).\n",
    "\n",
    "Aim for a model that balances bias and variance to achieve the lowest error on the test set.\n",
    "\n",
    "## **Practical Example**\n",
    "\n",
    "Suppose we try polynomial models up to the $10^{th}$-degree:\n",
    "\n",
    "1. For $1^{st}$-degree (linear): High training and validation errors (underfitting).\n",
    "2. For $10^{th}$-degree: Very low training error but high validation error (overfitting).\n",
    "3. For $3^{rd}$-degree: Training and validation errors are both reasonably low, indicating a good fit.\n",
    "\n",
    "The $3^{rd}$-degree polynomial might be the best choice because it balances complexity and generalization.\n",
    "\n",
    "---\n",
    "\n",
    "By experimenting and evaluating models using these principles, you can choose a model that generalizes well without falling into the trap of overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d7627c-e8d3-40ac-9c0e-1a713c219c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02ad67d-142b-402c-951e-3f7aa2611065",
   "metadata": {},
   "source": [
    "# Train, Validation, and Test Sets in Machine Learning\n",
    "\n",
    "In machine learning, splitting data into different sets ensures that the model is trained, tuned, and evaluated properly. The three main sets are: **Training Set**, **Validation Set**, and **Test Set**. Each serves a specific purpose:\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Training Set\n",
    "   **Purpose**:  \n",
    "   - The training set is used to train the model. It teaches the model to learn patterns and relationships in the data.\n",
    "\n",
    "   **Example**:  \n",
    "   - Suppose you are building a model to predict house prices. The training set might include features such as house size, location, and number of bedrooms, along with the corresponding prices.\n",
    "\n",
    "   **Key Point**:  \n",
    "   - The model learns by minimizing error on this data using optimization algorithms like gradient descent.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Validation Set\n",
    "   **Purpose**:  \n",
    "   - The validation set is used to tune the model's hyperparameters and assess the modelâ€™s performance during training.\n",
    "\n",
    "   **Example**:  \n",
    "   - After training on the training set, you test the model on the validation set to find the optimal learning rate or regularization strength.\n",
    "\n",
    "   **Key Point**:  \n",
    "   - It helps prevent overfitting by showing how well the model generalizes to unseen data during training. \n",
    "   - Hyperparameters such as the number of hidden layers or regularization strength are tuned using the validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Test Set\n",
    "   **Purpose**:  \n",
    "   - The test set is used to evaluate the model's final performance after all training and tuning are complete.\n",
    "\n",
    "   **Example**:  \n",
    "   - Once you have finalized the model and hyperparameters, you assess the model's performance on the test set, which it has never seen before.\n",
    "\n",
    "   **Key Point**:  \n",
    "   - It provides an unbiased estimate of the model's ability to generalize to new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## Analogy: Preparing for an Exam\n",
    "\n",
    "- **Training Set**: The notes and exercises you use to study and practice.\n",
    "- **Validation Set**: Mock exams you take to test your preparation and adjust your study strategy.\n",
    "- **Test Set**: The actual exam where your final understanding is assessed.\n",
    "\n",
    "---\n",
    "\n",
    "## Example Workflow\n",
    "\n",
    "Suppose you have 10,000 data points:\n",
    "\n",
    "1. **Training set**: 70% (7,000 points) used to train the model.\n",
    "2. **Validation set**: 20% (2,000 points) used to tune hyperparameters.\n",
    "3. **Test set**: 10% (1,000 points) used to assess the model's final performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Why All Three Are Necessary\n",
    "\n",
    "1. **Without a Test Set**: You cannot objectively measure the performance of the model.\n",
    "2. **Without a Validation Set**: The model may be overfitted during hyperparameter tuning.\n",
    "3. **Without a Training Set**: The model cannot learn from the data in the first place.\n",
    "\n",
    "This division ensures the model generalizes well to unseen data and avoids common issues like overfitting or underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9507355-6c78-4460-9001-f35b91da8b57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
