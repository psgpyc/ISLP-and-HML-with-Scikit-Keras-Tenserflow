{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "408c34c0-a5b9-4b2e-8306-3ff4d5ce8870",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=np.array([[1.00e+03, 1.46e+03, 0.00e+00],\n",
    "       [1.05e+03, 1.01e+03, 0.00e+00],\n",
    "       [1.09e+03, 8.54e+02, 0.00e+00],\n",
    "       [1.14e+03, 2.56e+03, 0.00e+00],\n",
    "       [1.18e+03, 4.62e+03, 0.00e+00],\n",
    "       [1.23e+03, 4.97e+03, 0.00e+00],\n",
    "       [1.27e+03, 2.39e+03, 0.00e+00],\n",
    "       [1.32e+03, 1.01e+02, 0.00e+00],\n",
    "       [1.36e+03, 7.29e+02, 0.00e+00],\n",
    "       [1.41e+03, 4.22e+03, 0.00e+00],\n",
    "       [1.45e+03, 4.30e+03, 0.00e+00],\n",
    "       [1.50e+03, 4.52e+02, 0.00e+00],\n",
    "       [1.54e+03, 2.76e+02, 0.00e+00],\n",
    "       [1.59e+03, 2.24e+03, 0.00e+00],\n",
    "       [1.63e+03, 2.76e+03, 0.00e+00],\n",
    "       [1.68e+03, 2.96e+03, 0.00e+00],\n",
    "       [1.72e+03, 3.99e+03, 0.00e+00],\n",
    "       [1.77e+03, 8.79e+02, 0.00e+00],\n",
    "       [1.81e+03, 3.42e+03, 0.00e+00],\n",
    "       [1.86e+03, 1.48e+03, 0.00e+00],\n",
    "       [1.90e+03, 1.28e+03, 0.00e+00],\n",
    "       [1.95e+03, 4.02e+02, 0.00e+00],\n",
    "       [1.99e+03, 1.11e+03, 0.00e+00],\n",
    "       [2.04e+03, 2.36e+03, 0.00e+00],\n",
    "       [2.09e+03, 7.79e+02, 1.00e+00],\n",
    "       [2.13e+03, 4.07e+03, 0.00e+00],\n",
    "       [2.18e+03, 9.55e+02, 0.00e+00],\n",
    "       [2.22e+03, 7.04e+02, 1.00e+00],\n",
    "       [2.27e+03, 4.85e+03, 0.00e+00],\n",
    "       [2.31e+03, 6.78e+02, 0.00e+00],\n",
    "       [2.36e+03, 1.18e+03, 1.00e+00],\n",
    "       [2.40e+03, 4.15e+03, 0.00e+00],\n",
    "       [2.45e+03, 4.87e+03, 0.00e+00],\n",
    "       [2.49e+03, 4.45e+03, 0.00e+00],\n",
    "       [2.54e+03, 4.42e+03, 0.00e+00],\n",
    "       [2.58e+03, 2.44e+03, 0.00e+00],\n",
    "       [2.63e+03, 4.37e+03, 0.00e+00],\n",
    "       [2.67e+03, 1.83e+03, 1.00e+00],\n",
    "       [2.72e+03, 1.73e+03, 0.00e+00],\n",
    "       [2.76e+03, 4.32e+03, 0.00e+00],\n",
    "       [2.81e+03, 2.71e+03, 0.00e+00],\n",
    "       [2.85e+03, 2.69e+03, 0.00e+00],\n",
    "       [2.90e+03, 4.75e+03, 0.00e+00],\n",
    "       [2.94e+03, 3.52e+02, 1.00e+00],\n",
    "       [2.99e+03, 1.41e+03, 0.00e+00],\n",
    "       [3.04e+03, 4.77e+02, 0.00e+00],\n",
    "       [3.08e+03, 2.86e+03, 0.00e+00],\n",
    "       [3.13e+03, 9.80e+02, 0.00e+00],\n",
    "       [3.17e+03, 4.65e+03, 0.00e+00],\n",
    "       [3.22e+03, 3.12e+03, 0.00e+00],\n",
    "       [3.26e+03, 2.46e+03, 0.00e+00],\n",
    "       [3.31e+03, 3.09e+03, 0.00e+00],\n",
    "       [3.35e+03, 2.99e+03, 0.00e+00],\n",
    "       [3.40e+03, 1.33e+03, 1.00e+00],\n",
    "       [3.44e+03, 8.29e+02, 1.00e+00],\n",
    "       [3.49e+03, 4.50e+03, 0.00e+00],\n",
    "       [3.53e+03, 4.55e+03, 0.00e+00],\n",
    "       [3.58e+03, 2.66e+03, 1.00e+00],\n",
    "       [3.62e+03, 5.00e+03, 0.00e+00],\n",
    "       [3.67e+03, 3.47e+03, 0.00e+00],\n",
    "       [3.71e+03, 2.91e+03, 0.00e+00],\n",
    "       [3.76e+03, 1.68e+03, 0.00e+00],\n",
    "       [3.80e+03, 1.96e+03, 0.00e+00],\n",
    "       [3.85e+03, 1.06e+03, 0.00e+00],\n",
    "       [3.89e+03, 4.27e+02, 0.00e+00],\n",
    "       [3.94e+03, 1.26e+02, 1.00e+00],\n",
    "       [3.98e+03, 3.19e+03, 0.00e+00],\n",
    "       [4.03e+03, 2.64e+03, 1.00e+00],\n",
    "       [4.08e+03, 1.21e+03, 1.00e+00],\n",
    "       [4.12e+03, 1.66e+03, 1.00e+00],\n",
    "       [4.17e+03, 1.36e+03, 0.00e+00],\n",
    "       [4.21e+03, 2.11e+03, 0.00e+00],\n",
    "       [4.26e+03, 4.60e+03, 0.00e+00],\n",
    "       [4.30e+03, 3.97e+03, 0.00e+00],\n",
    "       [4.35e+03, 4.17e+03, 0.00e+00],\n",
    "       [4.39e+03, 2.84e+03, 1.00e+00],\n",
    "       [4.44e+03, 3.02e+02, 1.00e+00],\n",
    "       [4.48e+03, 2.94e+03, 0.00e+00],\n",
    "       [4.53e+03, 2.34e+03, 1.00e+00],\n",
    "       [4.57e+03, 3.02e+03, 0.00e+00],\n",
    "       [4.62e+03, 3.87e+03, 0.00e+00],\n",
    "       [4.66e+03, 2.26e+03, 0.00e+00],\n",
    "       [4.71e+03, 2.04e+03, 0.00e+00],\n",
    "       [4.75e+03, 3.07e+03, 1.00e+00],\n",
    "       [4.80e+03, 4.80e+03, 0.00e+00],\n",
    "       [4.84e+03, 3.27e+02, 1.00e+00],\n",
    "       [4.89e+03, 2.06e+03, 0.00e+00],\n",
    "       [4.93e+03, 3.32e+03, 1.00e+00],\n",
    "       [4.98e+03, 4.70e+03, 0.00e+00],\n",
    "       [5.03e+03, 1.13e+03, 0.00e+00],\n",
    "       [5.07e+03, 2.49e+03, 1.00e+00],\n",
    "       [5.12e+03, 9.05e+02, 1.00e+00],\n",
    "       [5.16e+03, 4.05e+03, 0.00e+00],\n",
    "       [5.21e+03, 4.67e+03, 0.00e+00],\n",
    "       [5.25e+03, 3.84e+03, 0.00e+00],\n",
    "       [5.30e+03, 2.59e+03, 1.00e+00],\n",
    "       [5.34e+03, 4.90e+03, 0.00e+00],\n",
    "       [5.39e+03, 4.95e+03, 0.00e+00],\n",
    "       [5.43e+03, 3.72e+03, 1.00e+00],\n",
    "       [5.48e+03, 4.35e+03, 0.00e+00],\n",
    "       [5.52e+03, 1.88e+03, 1.00e+00],\n",
    "       [5.57e+03, 5.28e+02, 1.00e+00],\n",
    "       [5.61e+03, 2.29e+03, 1.00e+00],\n",
    "       [5.66e+03, 3.82e+03, 1.00e+00],\n",
    "       [5.70e+03, 5.03e+01, 1.00e+00],\n",
    "       [5.75e+03, 1.76e+03, 1.00e+00],\n",
    "       [5.79e+03, 2.14e+03, 1.00e+00],\n",
    "       [5.84e+03, 3.77e+03, 1.00e+00],\n",
    "       [5.88e+03, 1.51e+02, 1.00e+00],\n",
    "       [5.93e+03, 2.81e+03, 0.00e+00],\n",
    "       [5.97e+03, 0.00e+00, 1.00e+00],\n",
    "       [6.02e+03, 3.89e+03, 0.00e+00],\n",
    "       [6.07e+03, 1.93e+03, 1.00e+00],\n",
    "       [6.11e+03, 1.63e+03, 1.00e+00],\n",
    "       [6.16e+03, 1.38e+03, 1.00e+00],\n",
    "       [6.20e+03, 4.20e+03, 1.00e+00],\n",
    "       [6.25e+03, 2.21e+03, 1.00e+00],\n",
    "       [6.29e+03, 3.27e+03, 1.00e+00],\n",
    "       [6.34e+03, 1.16e+03, 1.00e+00],\n",
    "       [6.38e+03, 1.56e+03, 1.00e+00],\n",
    "       [6.43e+03, 1.86e+03, 1.00e+00],\n",
    "       [6.47e+03, 2.31e+03, 1.00e+00],\n",
    "       [6.52e+03, 3.69e+03, 0.00e+00],\n",
    "       [6.56e+03, 4.02e+03, 1.00e+00],\n",
    "       [6.61e+03, 3.59e+03, 1.00e+00],\n",
    "       [6.65e+03, 2.19e+03, 1.00e+00],\n",
    "       [6.70e+03, 4.52e+03, 0.00e+00],\n",
    "       [6.74e+03, 3.64e+03, 0.00e+00],\n",
    "       [6.79e+03, 4.12e+03, 0.00e+00],\n",
    "       [6.83e+03, 2.51e+02, 1.00e+00],\n",
    "       [6.88e+03, 8.04e+02, 1.00e+00],\n",
    "       [6.92e+03, 2.09e+03, 1.00e+00],\n",
    "       [6.97e+03, 4.57e+03, 1.00e+00],\n",
    "       [7.02e+03, 2.51e+03, 1.00e+00],\n",
    "       [7.06e+03, 3.14e+03, 1.00e+00],\n",
    "       [7.11e+03, 5.78e+02, 1.00e+00],\n",
    "       [7.15e+03, 3.17e+03, 1.00e+00],\n",
    "       [7.20e+03, 2.26e+02, 1.00e+00],\n",
    "       [7.24e+03, 4.27e+03, 1.00e+00],\n",
    "       [7.29e+03, 2.61e+03, 1.00e+00],\n",
    "       [7.33e+03, 3.79e+03, 1.00e+00],\n",
    "       [7.38e+03, 3.39e+03, 1.00e+00],\n",
    "       [7.42e+03, 2.79e+03, 1.00e+00],\n",
    "       [7.47e+03, 4.72e+03, 1.00e+00],\n",
    "       [7.51e+03, 1.61e+03, 1.00e+00],\n",
    "       [7.56e+03, 3.77e+02, 1.00e+00],\n",
    "       [7.60e+03, 1.03e+03, 1.00e+00],\n",
    "       [7.65e+03, 4.10e+03, 1.00e+00],\n",
    "       [7.69e+03, 2.74e+03, 1.00e+00],\n",
    "       [7.74e+03, 2.01e+03, 1.00e+00],\n",
    "       [7.78e+03, 1.31e+03, 1.00e+00],\n",
    "       [7.83e+03, 6.53e+02, 1.00e+00],\n",
    "       [7.87e+03, 1.91e+03, 1.00e+00],\n",
    "       [7.92e+03, 1.08e+03, 1.00e+00],\n",
    "       [7.96e+03, 6.03e+02, 1.00e+00],\n",
    "       [8.01e+03, 7.54e+01, 1.00e+00],\n",
    "       [8.06e+03, 4.25e+03, 1.00e+00],\n",
    "       [8.10e+03, 1.23e+03, 1.00e+00],\n",
    "       [8.15e+03, 3.74e+03, 1.00e+00],\n",
    "       [8.19e+03, 3.29e+03, 1.00e+00],\n",
    "       [8.24e+03, 4.77e+03, 1.00e+00],\n",
    "       [8.28e+03, 7.54e+02, 1.00e+00],\n",
    "       [8.33e+03, 3.04e+03, 1.00e+00],\n",
    "       [8.37e+03, 2.89e+03, 1.00e+00],\n",
    "       [8.42e+03, 4.40e+03, 1.00e+00],\n",
    "       [8.46e+03, 2.01e+02, 1.00e+00],\n",
    "       [8.51e+03, 1.51e+03, 1.00e+00],\n",
    "       [8.55e+03, 3.22e+03, 1.00e+00],\n",
    "       [8.60e+03, 2.51e+01, 1.00e+00],\n",
    "       [8.64e+03, 1.43e+03, 1.00e+00],\n",
    "       [8.69e+03, 5.53e+02, 1.00e+00],\n",
    "       [8.73e+03, 1.53e+03, 1.00e+00],\n",
    "       [8.78e+03, 1.58e+03, 1.00e+00],\n",
    "       [8.82e+03, 1.76e+02, 1.00e+00],\n",
    "       [8.87e+03, 4.92e+03, 1.00e+00],\n",
    "       [8.91e+03, 3.54e+03, 1.00e+00],\n",
    "       [8.96e+03, 2.16e+03, 1.00e+00],\n",
    "       [9.01e+03, 2.41e+03, 1.00e+00],\n",
    "       [9.05e+03, 1.71e+03, 1.00e+00],\n",
    "       [9.10e+03, 1.26e+03, 1.00e+00],\n",
    "       [9.14e+03, 3.57e+03, 1.00e+00],\n",
    "       [9.19e+03, 3.94e+03, 1.00e+00],\n",
    "       [9.23e+03, 3.92e+03, 1.00e+00],\n",
    "       [9.28e+03, 3.49e+03, 1.00e+00],\n",
    "       [9.32e+03, 3.67e+03, 1.00e+00],\n",
    "       [9.37e+03, 2.54e+03, 1.00e+00],\n",
    "       [9.41e+03, 5.03e+02, 1.00e+00],\n",
    "       [9.46e+03, 4.47e+03, 1.00e+00],\n",
    "       [9.50e+03, 6.28e+02, 1.00e+00],\n",
    "       [9.55e+03, 3.37e+03, 1.00e+00],\n",
    "       [9.59e+03, 1.78e+03, 1.00e+00],\n",
    "       [9.64e+03, 3.24e+03, 1.00e+00],\n",
    "       [9.68e+03, 3.62e+03, 1.00e+00],\n",
    "       [9.73e+03, 4.82e+03, 1.00e+00],\n",
    "       [9.77e+03, 1.98e+03, 1.00e+00],\n",
    "       [9.82e+03, 3.34e+03, 1.00e+00],\n",
    "       [9.86e+03, 3.44e+03, 1.00e+00],\n",
    "       [9.91e+03, 1.81e+03, 1.00e+00],\n",
    "       [9.95e+03, 3.52e+03, 1.00e+00],\n",
    "       [1.00e+04, 9.30e+02, 1.00e+00]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5781f956-c30f-43dc-aaf8-d627fbebbb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85f7c66c-15f3-411c-a750-24e378285c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b0c1281-1bb2-4dd7-8329-17a3c57be3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the inputs x is: (200, 2)\n",
      "the shape of the targets y is: (200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split the inputs and outputs into separate arrays\n",
    "x_bc = data[:,:-1]\n",
    "y_bc = data[:,-1]\n",
    "\n",
    "# Convert y into 2-D because the commands later will require it (x is already 2-D)\n",
    "y_bc = np.expand_dims(y_bc, axis=1)\n",
    "\n",
    "print(f\"the shape of the inputs x is: {x_bc.shape}\")\n",
    "print(f\"the shape of the targets y is: {y_bc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27e3812-f0ef-47d8-9ccd-e6eca4d74b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the training set (input) is: (120, 2)\n",
      "the shape of the training set (target) is: (120, 1)\n",
      "\n",
      "the shape of the cross validation set (input) is: (40, 2)\n",
      "the shape of the cross validation set (target) is: (40, 1)\n",
      "\n",
      "the shape of the test set (input) is: (40, 2)\n",
      "the shape of the test set (target) is: (40, 1)\n"
     ]
    }
   ],
   "source": [
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables.\n",
    "x_bc_train, x_, y_bc_train, y_ = train_test_split(x_bc, y_bc, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_bc_cv, x_bc_test, y_bc_cv, y_bc_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_\n",
    "\n",
    "print(f\"the shape of the training set (input) is: {x_bc_train.shape}\")\n",
    "print(f\"the shape of the training set (target) is: {y_bc_train.shape}\\n\")\n",
    "print(f\"the shape of the cross validation set (input) is: {x_bc_cv.shape}\")\n",
    "print(f\"the shape of the cross validation set (target) is: {y_bc_cv.shape}\\n\")\n",
    "print(f\"the shape of the test set (input) is: {x_bc_test.shape}\")\n",
    "print(f\"the shape of the test set (target) is: {y_bc_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f541ffe-b73b-4e72-bec0-8a409981818e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "\n",
    "# Initialize the class\n",
    "scaler_linear = StandardScaler()\n",
    "\n",
    "# Compute the mean and standard deviation of the training set then transform it\n",
    "x_bc_train_scaled = scaler_linear.fit_transform(x_bc_train)\n",
    "x_bc_cv_scaled = scaler_linear.transform(x_bc_cv)\n",
    "x_bc_test_scaled = scaler_linear.transform(x_bc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4712f16-ad02-4e96-ae96-ca773f338320",
   "metadata": {},
   "source": [
    "### Evaluating the error for classification models\n",
    "\n",
    "In the previous sections on regression models, you used the mean squared error to measure how well your model is doing. For classification, you can get a similar metric by getting the fraction of the data that the model has misclassified. For example, if your model made wrong predictions for 2 samples out of 5, then you will report an error of `40%` or `0.4`. The code below demonstrates this using a for-loop and also with Numpy's [`mean()`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a96e622-cd73-4c55-8717-e7a4eeb598c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities: [0.2 0.6 0.7 0.3 0.8]\n",
      "predictions with threshold=0.5: [0 1 1 0 1]\n",
      "targets: [1 1 1 1 1]\n",
      "fraction of misclassified data (for-loop): 0.4\n",
      "fraction of misclassified data (with np.mean()): 0.4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample model output\n",
    "probabilities = np.array([0.2, 0.6, 0.7, 0.3, 0.8])\n",
    "\n",
    "# Apply a threshold to the model output. If greater than 0.5, set to 1. Else 0.\n",
    "predictions = np.where(probabilities >= 0.5, 1, 0)\n",
    "\n",
    "# Ground truth labels\n",
    "ground_truth = np.array([1, 1, 1, 1, 1])\n",
    "\n",
    "# Initialize counter for misclassified data\n",
    "misclassified = 0\n",
    "\n",
    "# Get number of predictions\n",
    "num_predictions = len(predictions)\n",
    "\n",
    "# Loop over each prediction\n",
    "for i in range(num_predictions):\n",
    "    \n",
    "    # Check if it matches the ground truth\n",
    "    if predictions[i] != ground_truth[i]:\n",
    "        \n",
    "        # Add one to the counter if the prediction is wrong\n",
    "        misclassified += 1\n",
    "\n",
    "# Compute the fraction of the data that the model misclassified\n",
    "fraction_error = misclassified/num_predictions\n",
    "\n",
    "print(f\"probabilities: {probabilities}\")\n",
    "print(f\"predictions with threshold=0.5: {predictions}\")\n",
    "print(f\"targets: {ground_truth}\")\n",
    "print(f\"fraction of misclassified data (for-loop): {fraction_error}\")\n",
    "print(f\"fraction of misclassified data (with np.mean()): {np.mean(predictions != ground_truth)}\")\n",
    "\n",
    "\n",
    "np.sum(predictions == ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e794ca3-8f2d-418c-b7f6-712d6f054ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# units for each layer of each model\n",
    "units = [[25,15,1], [20,12,12,20,1], [32,16,8,4,12,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "774f31b0-8ab4-4593-8d49-0920b0d5e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.sequential.Sequential object at 0x1775f74f0>\n",
      "<keras.engine.sequential.Sequential object at 0x17792f250>\n",
      "<keras.engine.sequential.Sequential object at 0x17792d420>\n"
     ]
    }
   ],
   "source": [
    "nn_models = []\n",
    "for each in units:\n",
    "    dense_layers = []\n",
    "    for layers_units in range(len(each)):\n",
    "        if each[layers_units] == 1:\n",
    "            dense_layers.append(Dense(units=each[layers_units], activation=\"linear\"))\n",
    "        else:\n",
    "            dense_layers.append(Dense(units=each[layers_units], activation=\"relu\"))\n",
    "\n",
    "    model = Sequential(dense_layers)\n",
    "    print(model)\n",
    "    nn_models.append(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e482d900-3b4b-4d4b-bb8c-3cd94355aac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequential...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-20 13:39:50.099681: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "\n",
      "4/4 [==============================] - 0s 358us/step\n",
      "2/2 [==============================] - 0s 521us/step\n",
      "Training sequential_1...\n",
      "Done!\n",
      "\n",
      "4/4 [==============================] - 0s 386us/step\n",
      "2/2 [==============================] - 0s 637us/step\n",
      "Training sequential_2...\n",
      "Done!\n",
      "\n",
      "4/4 [==============================] - 0s 380us/step\n",
      "2/2 [==============================] - 0s 602us/step\n",
      "Model 1: Training Set Classification Error: 0.06667, CV Set Classification Error: 0.15000\n",
      "Model 2: Training Set Classification Error: 0.07500, CV Set Classification Error: 0.17500\n",
      "Model 3: Training Set Classification Error: 0.05833, CV Set Classification Error: 0.17500\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Build the models\n",
    "models_bc = nn_models\n",
    "\n",
    "# Loop over each model\n",
    "for model in models_bc:\n",
    "    \n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "\n",
    "    print(f\"Training {model.name}...\")\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_bc_train_scaled, y_bc_train,\n",
    "        epochs=200,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(\"Done!\\n\")\n",
    "    \n",
    "    # Set the threshold for classification\n",
    "    threshold = 0.5\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    yhat = model.predict(x_bc_train_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    train_error = np.mean(yhat != y_bc_train)\n",
    "    nn_train_error.append(train_error)\n",
    "\n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    yhat = model.predict(x_bc_cv_scaled)\n",
    "    yhat = tf.math.sigmoid(yhat)\n",
    "    yhat = np.where(yhat >= threshold, 1, 0)\n",
    "    cv_error = np.mean(yhat != y_bc_cv)\n",
    "    nn_cv_error.append(cv_error)\n",
    "\n",
    "# Print the result\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1}: Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
    "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b87038a-f97f-4ec6-9d4b-5df1c7ec4708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
