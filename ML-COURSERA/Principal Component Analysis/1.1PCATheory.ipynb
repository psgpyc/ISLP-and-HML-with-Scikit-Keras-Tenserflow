{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb040a3a-cfde-4c53-8f22-94b509b921dc",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a **dimensionality reduction technique** used in data analysis and machine learning to simplify datasets while retaining as much variability (information) as possible. It is particularly useful when dealing with high-dimensional data, where visualisation and computation can become challenging.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts of PCA\n",
    "\n",
    "1. **Dimensionality Reduction**:\n",
    "   PCA transforms a dataset with $n$ features (dimensions) into a smaller number $k$ of dimensions, where $k < n$. These new dimensions are called **principal components**.\n",
    "\n",
    "2. **Variance Maximisation**:\n",
    "   PCA finds directions (principal components) in the data that maximize the variance. The first principal component (PC1) captures the most variance, the second captures the next highest variance (orthogonal to PC1), and so on.\n",
    "\n",
    "3. **Uncorrelated Features**:\n",
    "   The principal components are **uncorrelated** (orthogonal to one another). This is particularly useful for avoiding redundancy in data.\n",
    "\n",
    "4. **Linear Transformation**:\n",
    "   PCA is a linear transformation methodâ€”it projects data into a new coordinate system defined by the principal components.\n",
    "\n",
    "---\n",
    "\n",
    "## Steps of PCA\n",
    "\n",
    "1. **Standardise the Data**:\n",
    "   PCA is sensitive to the scale of features. Standardise the data so that each feature has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "2. **Compute the Covariance Matrix**:\n",
    "   The covariance matrix captures the relationships between the features. It shows how much two features vary together.\n",
    "   \n",
    "   $$\n",
    "   \\text{Covariance Matrix} = \\frac{1}{n} X^T X\n",
    "   $$\n",
    "\n",
    "3. **Compute the Eigenvalues and Eigenvectors**:\n",
    "   The eigenvalues and eigenvectors of the covariance matrix determine the principal components. \n",
    "   - **Eigenvectors** represent the directions of the principal components.\n",
    "   - **Eigenvalues** represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Sort and Select Principal Components**:\n",
    "   Rank the eigenvalues in descending order and choose the top $k$ components (corresponding to the top $k$ eigenvectors).\n",
    "\n",
    "5. **Project the Data**:\n",
    "   Transform the original data into the new space using the selected eigenvectors. This gives the reduced data:\n",
    "   $$\n",
    "   Z = X W\n",
    "   $$\n",
    "   where $$X$$ is the original data, and $W$ contains the selected eigenvectors as columns.\n",
    "\n",
    "---\n",
    "\n",
    "## Example\n",
    "Suppose you have a dataset with two features, $X_1$ and $X_2$:\n",
    "- PCA will find the directions (principal components) that best describe the spread of the data.\n",
    "- The first principal component might be a diagonal line if the data forms an elongated cluster.\n",
    "- The second principal component will be orthogonal to the first, capturing less variance.\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of PCA\n",
    "\n",
    "- **Data Visualisation**: Reducing high-dimensional data to 2D or 3D for plotting.\n",
    "- **Noise Reduction**: Removing less significant components can denoise the data.\n",
    "- **Feature Reduction**: Simplify models by working with fewer features.\n",
    "- **Compression**: Reduce data size without significant loss of information.\n",
    "\n",
    "---\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Linear Assumptions**: PCA works best when relationships between features are linear.\n",
    "2. **Interpretability**: The principal components are combinations of original features, making them harder to interpret.\n",
    "3. **Loss of Information**: Some variance is inevitably lost when reducing dimensions.\n",
    "\n",
    "---\n",
    "\n",
    "In summary, PCA is a powerful tool for simplifying datasets and uncovering the key patterns in high-dimensional data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498a544-4a61-485b-88d3-fc177bbd46fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
